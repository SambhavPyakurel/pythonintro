{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f426e4dd-a4cf-4836-b222-4ce0978a955d",
   "metadata": {},
   "source": [
    "#### Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some algorithms that are not affected by missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fd81a0-a7ed-4cad-97ff-dc887278ca7b",
   "metadata": {},
   "source": [
    "Missing values in a dataset refer to instances where no data is available for a particular feature or attribute. Some common reasons for missing values are:\n",
    "\n",
    "- Data was simply not collected or recorded for some instances\n",
    "- Data was lost or corrupted \n",
    "- Participants skipped certain questions in a survey\n",
    "\n",
    "It is essential to handle missing values for the following reasons:\n",
    "\n",
    "1. Many machine learning algorithms cannot handle missing values and will throw errors if values are missing.\n",
    "\n",
    "2. The presence of missing values can negatively impact the performance of algorithms that can handle them, especially if the missing values are not random.\n",
    "\n",
    "3. Imputing values for missing data can improve the performance of some algorithms.\n",
    "\n",
    "Algorithms that are not affected by missing values include:\n",
    "\n",
    "- Decision trees algorithms like Random Forest and Gradient Boosted Trees\n",
    "- Instance-based algorithms like K-Nearest Neighbors\n",
    "- Some deep learning architectures like Convolutional Neural Networks\n",
    "\n",
    "To handle missing values, common approaches are:\n",
    "\n",
    "- Drop the instances with missing values \n",
    "- Impute values using mean, median or mode of the feature\n",
    "- Impute using more sophisticated techniques like K-nearest neighbor imputation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4c2a97-5285-48ce-9216-c0130bf64496",
   "metadata": {},
   "source": [
    "#### Q2: List down techniques used to handle missing data. Give an example of each with python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "babc9b9b-3fd4-4250-8a3c-c4935fa7a3d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>embarked</th>\n",
       "      <th>class</th>\n",
       "      <th>who</th>\n",
       "      <th>adult_male</th>\n",
       "      <th>deck</th>\n",
       "      <th>embark_town</th>\n",
       "      <th>alive</th>\n",
       "      <th>alone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>S</td>\n",
       "      <td>Second</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>S</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>B</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>C</td>\n",
       "      <td>First</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>C</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>Q</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Queenstown</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     survived  pclass     sex   age  sibsp  parch     fare embarked   class  \\\n",
       "0           0       3    male  22.0      1      0   7.2500        S   Third   \n",
       "1           1       1  female  38.0      1      0  71.2833        C   First   \n",
       "2           1       3  female  26.0      0      0   7.9250        S   Third   \n",
       "3           1       1  female  35.0      1      0  53.1000        S   First   \n",
       "4           0       3    male  35.0      0      0   8.0500        S   Third   \n",
       "..        ...     ...     ...   ...    ...    ...      ...      ...     ...   \n",
       "886         0       2    male  27.0      0      0  13.0000        S  Second   \n",
       "887         1       1  female  19.0      0      0  30.0000        S   First   \n",
       "888         0       3  female   NaN      1      2  23.4500        S   Third   \n",
       "889         1       1    male  26.0      0      0  30.0000        C   First   \n",
       "890         0       3    male  32.0      0      0   7.7500        Q   Third   \n",
       "\n",
       "       who  adult_male deck  embark_town alive  alone  \n",
       "0      man        True  NaN  Southampton    no  False  \n",
       "1    woman       False    C    Cherbourg   yes  False  \n",
       "2    woman       False  NaN  Southampton   yes   True  \n",
       "3    woman       False    C  Southampton   yes  False  \n",
       "4      man        True  NaN  Southampton    no   True  \n",
       "..     ...         ...  ...          ...   ...    ...  \n",
       "886    man        True  NaN  Southampton    no   True  \n",
       "887  woman       False    B  Southampton   yes   True  \n",
       "888  woman       False  NaN  Southampton    no  False  \n",
       "889    man        True    C    Cherbourg   yes   True  \n",
       "890    man        True  NaN   Queenstown    no   True  \n",
       "\n",
       "[891 rows x 15 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "df = sns.load_dataset('titanic')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "88c388a2-7ea2-4403-9a67-898866378b38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      22.0\n",
       "1      38.0\n",
       "2      26.0\n",
       "3      35.0\n",
       "4      35.0\n",
       "       ... \n",
       "886    27.0\n",
       "887    19.0\n",
       "888    29.0\n",
       "889    26.0\n",
       "890    32.0\n",
       "Name: age, Length: 891, dtype: float64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Mean Imputation\n",
    "df['age'].fillna(int(df['age'].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5f82aa7d-eecb-47ce-a39f-0c714d75f33e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      22.0\n",
       "1      38.0\n",
       "2      26.0\n",
       "3      35.0\n",
       "4      35.0\n",
       "       ... \n",
       "886    27.0\n",
       "887    19.0\n",
       "888    28.0\n",
       "889    26.0\n",
       "890    32.0\n",
       "Name: age, Length: 891, dtype: float64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Median Imputation\n",
    "df['age'].fillna(int(df['age'].median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3b5adcb8-2448-409c-ae05-e86dee02a2be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      S\n",
       "1      C\n",
       "2      S\n",
       "3      S\n",
       "4      S\n",
       "      ..\n",
       "886    S\n",
       "887    S\n",
       "888    S\n",
       "889    C\n",
       "890    Q\n",
       "Name: embarked, Length: 891, dtype: object"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Mode Imputation\n",
    "df['embarked'].fillna(df[df['embarked'].notna()]['embarked'].mode()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348046b5-de3d-428f-bf16-f48271c58514",
   "metadata": {},
   "source": [
    "#### Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d020ad6-ead8-401c-a519-8516dbedce63",
   "metadata": {},
   "source": [
    "Imbalanced data refers to datasets where the classes are not roughly equal in their representation. One class may have significantly more data than the other class.\n",
    "\n",
    "For example:\n",
    "- Fraud detection datasets often have much fewer fraud instances compared to non-fraud transactions\n",
    "- Medical datasets often have many more healthy patients compared to patients with the disease\n",
    "\n",
    "If imbalanced data is not handled properly, the following can happen:\n",
    "\n",
    "1. Models tend to be biased towards the majority class - Since the majority class has more examples, models learn to just predict the majority class for all examples. This results in high accuracy but poor performance on the minority class.\n",
    "\n",
    "2. Failure to detect the minority class - The minority class ends up being overlooked since models learn to mainly predict the majority class.\n",
    "\n",
    "3. Unreliable performance metrics - Accuracy is not a good metric for imbalanced data since the model can achieve high accuracy just by predicting the majority class. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6706062-f18c-4e8f-87fb-1923be3bf263",
   "metadata": {},
   "source": [
    "#### Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down-sampling are required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0604a18e-6596-4e91-be43-30be375ced00",
   "metadata": {},
   "source": [
    "Up-sampling and down-sampling are two techniques used to deal with imbalanced data.\n",
    "\n",
    "Up-sampling involves increasing the number of samples from the minority class, typically by creating synthetic samples. This helps make the minority and majority classes more balanced.\n",
    "\n",
    "Down-sampling involves reducing the number of samples from the majority class. This is done by either random removal of samples or more intelligent techniques.\n",
    "\n",
    "Up-sampling is required when:\n",
    "\n",
    "- The minority class represents the class of interest, e.g. fraud transactions. Increasing the number of minority class samples helps models learn better from this class.\n",
    "\n",
    "- The dataset is very small and up-sampling can help generate more training data.\n",
    "\n",
    "An example of up-sampling is SMOTE, which generates synthetic samples for the minority class.\n",
    "\n",
    "Down-sampling is required when:\n",
    "\n",
    "- The majority class has too many samples, causing models to be biased towards it \n",
    "- Training models on the entire majority class is computationally expensive\n",
    "\n",
    "An example of down-sampling is randomly removing majority class samples till a desired balance is achieved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76206d89-9786-4d7a-a757-73b653bf96c1",
   "metadata": {},
   "source": [
    "#### Q5: What is data Augmentation? Explain SMOTE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81035fdd-a1af-4f75-96c0-782fe779bc60",
   "metadata": {},
   "source": [
    "Data augmentation refers to techniques used to artificially expand the size of a training dataset by creating modified versions of existing samples. This helps address issues like:\n",
    "\n",
    "- Limited training data \n",
    "- Overfitting to the available data\n",
    "\n",
    "SMOTE (Synthetic Minority Over-sampling TEchnique) is a specific data augmentation technique used for imbalanced classification problems. It works by generating new synthetic samples for the minority class to balance the class distribution. \n",
    "\n",
    "How it works:\n",
    "\n",
    "1. For each minority class sample, SMOTE finds its k nearest neighbors, typically from the same class.\n",
    "\n",
    "2. It then randomly chooses one of the neighbors. \n",
    "\n",
    "3. A new synthetic sample is generated along the line segment joining the original sample and the chosen neighbor.\n",
    "\n",
    "For example, if k=5, for a given minority class sample A:\n",
    "\n",
    "- SMOTE finds the 5 nearest samples to A from the same minority class\n",
    "- It randomly chooses one of these 5 samples, say B\n",
    "- It then generates a new synthetic sample somewhere along the line segment between A and B\n",
    "\n",
    "This process is repeated until the desired balance between the minority and majority class is achieved.\n",
    "\n",
    "\n",
    "Overall, data augmentation techniques like SMOTE help generate more training data and improve model performance, especially on small and imbalanced datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277898f4-8221-4a14-8327-b77665281d07",
   "metadata": {},
   "source": [
    "#### Q6: What are outliers in a dataset? Why is it essential to handle outliers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d01ae0-0845-4d91-bcbd-b795653a7a7a",
   "metadata": {},
   "source": [
    "Outliers are data points that are very different from the rest of the data in a dataset. They lie outside the overall pattern or distribution of the data.\n",
    "\n",
    "It is important to handle outliers because:\n",
    "\n",
    "1. They can significantly skew the results of statistical analyses. Common statistics like mean, median and standard deviation can be heavily influenced by even a single outlier, distorting any conclusions drawn from the data.\n",
    "\n",
    "2. They can mask the true signal or pattern in the data. The presence of outliers can obscure the general trend or relationship that exists in the majority of the data. \n",
    "\n",
    "3. Machine learning models can be negatively impacted. Models trained on data with outliers can lead to poor performance when deployed since most real-world data will not contain such extreme values.\n",
    "\n",
    "4. They can indicate data errors. Outliers may be the result of data entry errors, measurement errors or other issues that need to be addressed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd8d376-cb70-403d-8b58-de552f7bcb24",
   "metadata": {},
   "source": [
    "#### Q7: You are working on a project that requires analyzing customer data. However, you notice that some of the data is missing. What are some techniques you can use to handle the missing data in your analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63203bed-29fc-47f9-b7ca-70ce844e20ae",
   "metadata": {},
   "source": [
    "1. Case deletion - Simply removing all cases with any missing data. However, this can significantly reduce the sample size.\n",
    "\n",
    "2. Mean imputation - Replacing missing data with the mean or average of the available data for that variable. Simple but can distort your distributions.\n",
    "\n",
    "3. Median imputation - Similar to mean imputation but uses the median instead. More robust to outliers.\n",
    "\n",
    "4. Mode imputatiom - for categorical data, simply replacing missing data with mode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b78ab42-fab4-4944-8762-65d4e1fa1b12",
   "metadata": {},
   "source": [
    "#### Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are some strategies you can use to determine if the missing data is missing at random or if there is a pattern to the missing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00383ac-7a24-4a2d-a884-76de72a7cdd2",
   "metadata": {},
   "source": [
    "Here are some strategies to determine if data is missing at random or in a systematic pattern:\n",
    "\n",
    "1. Compare variables with missing data to complete cases. Look for statistically significant differences in the mean, median, or distribution of other variables between cases with missing data and complete cases. If differences exist, the data is likely missing systematically.\n",
    "\n",
    "2. Examine patterns in the missing data. Look for correlations between variables with high amounts of missing data. Missing data that clusters or co-occurs may indicate a systematic pattern.  \n",
    "\n",
    "3. Perform chi-square tests of independence between variables with missing data and other categorical variables. A statistically significant result suggests the missing data is not random.\n",
    "\n",
    "4. Plot variables with missing data against other variables. Look for any trends or clusters in the plot that exclude cases with missing data. This could indicate a non-random pattern.\n",
    "\n",
    "5. Build predictive models to identify factors associated with missing data. If certain variables strongly predict which cases have missing data, that suggests a systematic cause for the missingness.\n",
    "\n",
    "6. Look for logical explanations for non-random missing data based on your domain knowledge. Consider data collection methods, question wording, time periods, etc. This contextual information can aid your analysis.\n",
    "\n",
    "7. Compare missing data rates across subgroups. If certain subgroups (e.g. demographic groups) have significantly higher rates of missing data, that points to a non-random pattern.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f5d017-ca78-462c-a658-b7e16d5dbe53",
   "metadata": {},
   "source": [
    "#### Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the dataset do not have the condition of interest, while a small percentage do. What are some strategies you can use to evaluate the performance of your machine learning model on this imbalanced dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2baa989-ad14-4bc4-abd5-27814001b4be",
   "metadata": {},
   "source": [
    "Here are some strategies used to evaluate the performance of your machine learning model on this imbalanced dataset:\n",
    "\n",
    "1. Upsampling: Adding duplicates or synthetically generated samples of the diseased class to match the healthy class. This can improve minority class prediction performance.\n",
    "\n",
    "2. Downsampling: Remove some healthy class samples at random to match the number of diseased class. This can make the classes balanced for model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8d486e-f6aa-408c-a35c-418ce90edca9",
   "metadata": {},
   "source": [
    "#### Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to balance the dataset and down-sample the majority class?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96d0e46-d98c-4945-ac1e-e5622f5bd659",
   "metadata": {},
   "source": [
    "Here are a few methods to downsample the majority satisfied customers class in your customer satisfaction dataset:\n",
    "\n",
    "1. Random undersampling: Randomly remove majority class examples at random until the classes are balanced. This is simple but can discard potentially useful data.\n",
    "\n",
    "2. Tomek links: Identify data points from the majority class that are very close to minority class examples, known as Tomek links. Remove one data point from each Tomek link, preferentially keeping minority class examples. This focuses the downsampling on data points that are hardest to classify.\n",
    "\n",
    "3. Cluster-based undersampling: Group majority class examples into clusters and randomly remove clusters until a balance is achieved. This preserves the underlying distribution of the data better than random undersampling.\n",
    "\n",
    "4. One-sided selection: Sort the majority class examples by their distance from the minority class, and remove examples that are closest. This preferentially retains examples that are \"further away\" from the minority class.\n",
    "\n",
    "5. Condensed nearest neighbor: Identify majority class examples that have only examples of the same class as their nearest neighbors, and remove them. This leaves data points that are more informative for distinguishing the classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8d5b0d-61b4-49f4-b144-0f49b1221874",
   "metadata": {},
   "source": [
    "#### Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a project that requires you to estimate the occurrence of a rare event. What methods can you employ to balance the dataset and up-sample the minority class?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e188e0-e2b0-45a2-960b-48a5ceb91a8c",
   "metadata": {},
   "source": [
    "Here are some methods  to upsample the minority class of occurrences in your imbalanced dataset:\n",
    "\n",
    "1. Random duplication: Simply duplicate random minority class examples at random until the classes are balanced. This is simple but can lead to overfitting since some examples are repeated multiple times.\n",
    "\n",
    "2. SMOTE: Synthetic Minority Oversampling Technique generates synthetic minority class examples rather than duplicating existing data. It works by identifying minority class examples that are close together, and generating new data along the line segment joining any/all of the k closest neighbors. \n",
    "\n",
    "3. ADASYN:  Advanced Synthetic Sampling generates different numbers of synthetic examples for different minority class examples, based on how \"hard\" they are to learn. It focuses more synthetic data around minority examples that are hard to learn.\n",
    "\n",
    "4. Cluster-based oversampling: Group minority class examples into clusters and generate synthetic samples for each cluster. This helps generate more varied synthetic data that captures some of the underlying structure of the minority class.\n",
    "\n",
    "5. Gaussian distribution oversampling: Generate synthetic data from a Gaussian distribution centered around existing minority class examples. The width of the distribution controls how similar synthetic data is to existing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcc8ad7-96a5-482f-83c8-4aec309dec47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
