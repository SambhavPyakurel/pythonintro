{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6be080a1-6615-4933-9a59-1268d446af55",
   "metadata": {},
   "source": [
    "#### Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defc9ebc-7fa8-4e91-bc79-e14afef4a70b",
   "metadata": {},
   "source": [
    "Min-Max scaling is a simple technique to scale the range of features in a dataset to a specific range, typically 0 to 1. It is used in data preprocessing before applying machine learning algorithms.\n",
    "\n",
    "It works by transforming the features according to the following formula:\n",
    "\n",
    "            x' = (x - x_min) / (x_max - x_min)\n",
    "\n",
    "where x' is the transformed value, x is the original value, and x_min and x_max are the minimum and maximum values for that feature in the dataset.\n",
    "\n",
    "For example, if we have a feature that has a range from -10 to +10, we can scale it to the range of 0 to 1 using Min-Max scaling:\n",
    "\n",
    "- For a value of -5, the scaled value would be:\n",
    "(-5 - (-10)) / (10 - (-10)) = 0.25\n",
    "\n",
    "- For a value of 0, the scaled value would be:\n",
    "(0 - (-10)) / (10 - (-10)) = 0.5\n",
    "\n",
    "- For a value of 10, the scaled value would be:\n",
    "(10 - (-10)) / (10 - (-10)) = 1\n",
    "\n",
    "Min-Max scaling is useful because it transforms the features to a common scale, which can improve the performance and speed of convergence of machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed74b1b-4389-4a35-b567-a97b47f4589b",
   "metadata": {},
   "source": [
    "#### Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e121b7-04f9-45b6-bbde-ee1a07ea54eb",
   "metadata": {},
   "source": [
    "The Unit Vector technique is another way to scale features to a range, typically 0 to 1. It differs from Min-Max scaling in the following way:\n",
    "\n",
    "- Min-Max scaling rescales the features linearly while preserving the relationships between original values. \n",
    "- Unit Vector scaling rescales the features such that each feature vector has a length of 1 after scaling. This means the relationships between original values may change after scaling.\n",
    "\n",
    "It works by scaling each feature according to this formula:\n",
    "\n",
    "x' = x / |x|\n",
    "\n",
    "Where x' is the scaled value and |x| is the length magnitude of the feature vector x.\n",
    "\n",
    "For example, if we have a feature vector [2, 3]:\n",
    "\n",
    "- The length of this vector is √(2^2 + 3^2) = √13 \n",
    "- To scale this vector to unit length, we divide each element by the overall length:\n",
    "\n",
    "Scaled vector = [2/√13, 3/√13] \n",
    "            = [0.43, 0.65]\n",
    "\n",
    "So the original vector [2, 3] is scaled to the unit vector [0.43, 0.65].\n",
    "\n",
    "The benefit of Unit Vector scaling is that it results in feature vectors of equal length, which can be useful for some machine learning algorithms. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc5512b-70d9-405a-991e-af9adc614aee",
   "metadata": {},
   "source": [
    "#### Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0e5004-c8fa-479a-8f9c-e22b5d98fc1e",
   "metadata": {},
   "source": [
    "Principal Component Analysis(PCA) is a technique for performing dimensionality reduction. It works by converting a set of features (which could be correlated) into a set of linearly uncorrelated values called principal components.\n",
    "\n",
    "The principal components are ordered such that the first principal component has the largest possible variance (i.e. accounts for as much of the variability in the data as possible), and each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to (i.e. uncorrelated with) the preceding components.\n",
    "\n",
    "This means that the first principal component will capture the most information (variance) in your dataset, then the second principal component will have the next highest variance, and so on.\n",
    "\n",
    "By keeping only the k components with the highest variance, we can reduce the dimensionality of the dataset while retaining most of the information (the variance). This is helpful when we have a large number of features, many of which might be correlated or redundant.\n",
    "\n",
    "For example, lets say we have a dataset with 3 features: length, width, and area. Width and area will likely be correlated.\n",
    "\n",
    "Running PCA on this dataset, the first principal component may represent a combination of length and width, while the second principal component represents area.\n",
    "\n",
    "We could then reduce the dimension to 1 (instead of the original 3) by keeping only the first principal component. This would reduce the number of features while retaining most of the information since the first principal component has the highest variance.\n",
    "\n",
    "In summary, PCA finds the directions of maximum variance in high-dimensional data and projects it onto a lower dimensional space, resulting in dimensionality reduction while preserving as much information as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99c9f80-76ab-42db-ab3d-587cc3e91b49",
   "metadata": {},
   "source": [
    "#### Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd484e9-ad81-472d-ad4f-27a9ba709073",
   "metadata": {},
   "source": [
    "PCA and feature extraction are closely related concepts. PCA can be used as a technique for feature extraction.\n",
    "\n",
    "PCA transforms a set of features into a set of linearly uncorrelated principal components. The principal components are ordered such that the first few components capture most of the variability (information) in the data.\n",
    "\n",
    "This means that the first few principal components can act as a compressed, lower dimensional representation of the original features - and thus can be used as the new 'features' in place of the original features. This is the basic idea of using PCA for feature extraction.\n",
    "\n",
    "In practice, we do the following:\n",
    "\n",
    "1) Run PCA on the original features to compute the principal components\n",
    "2) Choose the first k principal components that capture most of the variance (say 95% or more)\n",
    "3) Use these k principal components as the new 'features' for the dataset\n",
    "\n",
    "For example, say we have a dataset with 10 features (F1 to F10). We run PCA and find that the first 3 principal components (PC1, PC2, PC3) capture 95% of the variance in the data.\n",
    "\n",
    "We can then use PC1, PC2 and PC3 as the new features to represent our dataset, instead of the original 10 features. This 'extracts' 3 new features from the original 10 features, while compressing most of the information into those 3 features.\n",
    "\n",
    "The benefit of this approach is that the new features are uncorrelated and capture the most important information, which can improve the performance and interpretability of machine learning models trained on these features.\n",
    "\n",
    "So in summary, PCA is a technique for dimensionality reduction and feature extraction. It works by transforming the original features into a set of uncorrelated principal components, and the first few components can then be used as the new 'extracted features'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e74ce4-1a9a-406e-9cbd-d64367dd2d59",
   "metadata": {},
   "source": [
    "#### Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80b995d-440f-44e5-9308-a54d8c212d4b",
   "metadata": {},
   "source": [
    "Here is how I would use Min-Max scaling to preprocess the data for a food delivery recommendation system:\n",
    "\n",
    "1. Identify the features that need to be scaled, such as price, rating, and delivery time. \n",
    "\n",
    "These features have different units and ranges, so scaling them will help make them comparable.\n",
    "\n",
    "2. Determine the minimum and maximum values for each feature in the dataset. For example:\n",
    "\n",
    "- Price: min = \\\\$5, max = $50\n",
    "- Rating: min = 1, max = 5  \n",
    "- Delivery time: min = 10 mins, max = 60 mins\n",
    "\n",
    "3. Apply the Min-Max scaling formula to each feature for every data point:\n",
    "\n",
    "x' = (x - x_min) / (x_max - x_min)\n",
    "\n",
    "Where:\n",
    "- x' is the scaled value  \n",
    "- x is the original value   \n",
    "- x_min is the minimum value for that feature\n",
    "- x_max is the maximum value for that feature\n",
    "\n",
    "4. For example, to scale a price of $20:\n",
    "\n",
    "x' = (\\\\$20 - $5) / (\\\\$50 - $5) = 0.75\n",
    "\n",
    "So the scaled price would be 0.75.\n",
    "\n",
    "5. Apply this scaling to all data points for the price, rating, and delivery time features. The scaled values will now range from 0 to 1.\n",
    "\n",
    "6. Use the scaled features to build and train the recommendation model. The scaled features will allow the model to learn similarities and differences between data points more effectively.\n",
    "\n",
    "7. When making predictions, first scale any new data using the minimum and maximum values determined earlier, then provide the scaled values to the model for recommendations.\n",
    "\n",
    "So in summary, Min-Max scaling the features will transform them to a common scale, which can help the recommendation model perform better by making the differences and similarities between data points more apparent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb77028f-81dc-4471-9c4e-3707fc0f810f",
   "metadata": {},
   "source": [
    "#### Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e93066a-acb5-438c-a61a-fe95897352e7",
   "metadata": {},
   "source": [
    "Here is how I would use PCA to reduce the dimensionality of the dataset for predicting stock prices:\n",
    "\n",
    "1. Identify the features in the dataset.As mentioned there are many features, including company financial data and market trends. \n",
    "\n",
    "2. Run PCA on the dataset to determine the principal components. The PCA algorithm will analyze the variance and covariance in the features to find the principal components - the linear combinations of features that capture the most information.\n",
    "\n",
    "3. Determine how many principal components to retain based on the amount of variance they capture. For example, you may retain the first 10 principal components that capture at least 95% of the variance in the dataset.\n",
    "\n",
    "4. The principal components you retain will become the new features for the dataset, replacing the original features. These principal components will be linearly uncorrelated and will capture most of the information and variance in the original features.\n",
    "\n",
    "5. Use the principal components as inputs to build your stock price prediction model. Since there are now fewer features (the principal components), the model will be less complex and require less computation.\n",
    "\n",
    "6. When making predictions on new data, first project that data onto the principal components to obtain the corresponding principal component scores. Provide these scores as inputs to the model to make stock price predictions.\n",
    "\n",
    "7. Evaluate the performance of the model using these reduced dimensionality principal components as features, compared to using all the original features. In many cases, performance remains similar while reducing the number of features significantly.\n",
    "\n",
    "In summary, by applying PCA to reduce the dimensionality of the dataset, you can compress the many original features into a smaller number of principal components. These components will capture most of the information and variance in the original features, allowing you to build an accurate but simpler stock price prediction model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ee4e7f-862d-4a09-a4a8-bee2cbacc7f3",
   "metadata": {},
   "source": [
    "#### Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "57e7f4c0-2eb4-4d8b-9508-a54aa7e6756c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.        ]\n",
      " [-0.57894737]\n",
      " [-0.05263158]\n",
      " [ 0.47368421]\n",
      " [ 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "data = [1, 5, 10, 15, 20]\n",
    "data_array = np.array(data).reshape(-1, 1)\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaled = scaler.fit_transform(data_array)\n",
    "print(scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072f88d1-9741-4dcf-b455-b0a0976b3296",
   "metadata": {},
   "source": [
    "#### Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879c9aa3-34a3-4e59-91b7-051cf04739d0",
   "metadata": {},
   "source": [
    "Implementation of PCA would involve:\n",
    "\n",
    "- Gathering the data - which I do not have for this hypothetical dataset.\n",
    "- Performing PCA using a library like sklearn - calculating the principal components and their variance explained.\n",
    "- Analyzing the variance explained plot to determine how many principal components capture a sufficient amount of variance - likely somewhere between 70-90%.\n",
    "- Selecting that number of principal components (likely 2-3) and transforming the original data using only those - principal components.\n",
    "\n",
    "\n",
    "I would retain 2-3 principal components for this dataset:\n",
    "\n",
    "Reasons:\n",
    "\n",
    "1. The dataset has 5 features, so at most we can extract 5 principal components. Retaining all 5 would not reduce dimensionality. \n",
    "\n",
    "2. Retaining just 1 principal component would likely lose too much information. It would represent the overall \"health\" or \"body profile\" but would lose the details of the individual features.\n",
    "\n",
    "3. 2-3 principal components would likely capture the majority of the variance in the data, while reducing dimensionality by 40-60%. This works well as a dimensionality reduction technique for visualization and as input to some machine learning models.\n",
    "\n",
    "4. Empirically, for datasets with around 5 features, retaining 2-3 principal components often works well, capturing around 70-90% of the variance while providing a good reduction in dimensionality.\n",
    "\n",
    "So in summary, 2-3 principal components for this 5 feature dataset would provide a good balance of dimensionality reduction while retaining the most important information and variance in the data. Then the choice of how many (2 vs 3) could be tuned based on how much variance you want to capture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552a1547-c8ec-488e-966c-ce888da7b016",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
