{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a73fbfdb-c6c9-486f-b690-98b3dbac5ffb",
   "metadata": {},
   "source": [
    "#### Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an example of each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787c0214-eec0-4c12-94a0-9be9d24b19ba",
   "metadata": {},
   "source": [
    "- Simple Linear Regression: It involves a relationship between two variables, where one is considered the predictor (independent variable) and the other is the response (dependent variable). The goal is to find the best-fitting line that minimizes the sum of squared differences between the observed and predicted values.\n",
    "   - Example: Predicting a student's test score (response) based on the number of hours they studied (predictor).\n",
    "   \n",
    "- Multiple Linear Regression: It extends the concept of linear regression to include multiple predictors. It models the relationship between a dependent variable and two or more independent variables, assuming a linear relationship. The goal is to find the best-fitting hyperplane in a higher-dimensional space.\n",
    "   - Example: Predicting a house's sale price (response) based on its area, number of bedrooms, and location (predictors)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa1f2e7-ccab-4e12-8b9a-7faa5f14b041",
   "metadata": {},
   "source": [
    "#### Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in a given dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd40f8e-6873-4325-8917-f488c32ecc43",
   "metadata": {},
   "source": [
    " **Assumptions of Linear Regression:**\n",
    "   - Linearity: The relationship between predictors and the response is linear.\n",
    "   - Independence: Residuals (differences between observed and predicted values) are independent of each other.\n",
    "   - Homoscedasticity: Residuals have constant variance across all levels of predictors.\n",
    "   - Normality: Residuals are normally distributed.\n",
    "   \n",
    "**These assumptions can be checked using:**\n",
    "   - residual plots\n",
    "   - normal probability plots\n",
    "   - statistical tests such as the Shapiro-Wilk test or the Kolmogorov-Smirnov test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee432ade-242d-466b-bcf5-467798f1818e",
   "metadata": {},
   "source": [
    "#### Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using a real-world scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb237a7-d656-4f8c-8b6b-c5f3dcfadd1d",
   "metadata": {},
   "source": [
    "In a simple linear regression model (y = mx + b), the slope (m) represents the change in the dependent variable (y) for a unit change in the independent variable (x). The intercept (b) represents the value of y when x is 0.\n",
    "  - Example: For every additional hour a student studies, their test score is expected to increase by 'm' points. The intercept 'b' represents the expected test score when the student didn't study at all.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48010f06-51ab-47db-901c-e7a8b846c2a0",
   "metadata": {},
   "source": [
    "#### Q4. Explain the concept of gradient descent. How is it used in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fb7e80-2433-4757-9804-8ba750d5b807",
   "metadata": {},
   "source": [
    "**Gradient Descent:**\n",
    "   Gradient descent is an optimization algorithm used to minimize the cost (or loss) function of a model by adjusting its parameters iteratively. It works by calculating the gradient (derivative) of the cost function with respect to the model parameters and updating the parameters in the opposite direction of the gradient to reach the minimum. It's commonly used in training machine learning models to find the best-fitting parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b202150-f6c0-49d1-91a2-4deb8523edd0",
   "metadata": {},
   "source": [
    "#### Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce93901-46b9-451f-990e-253bbc2f3924",
   "metadata": {},
   "source": [
    "**Multiple Linear Regression:**\n",
    "   Multiple linear regression involves modeling the relationship between a dependent variable and multiple independent variables. The model equation is: y = b0 + b1x1 + b2x2 + ... + bnxn, where y is the dependent variable, xi's are independent variables, and b's are the coefficients representing the effect of each independent variable on the dependent variable.\n",
    "   \n",
    "Multiple linear regression models the relationship between a dependent variable and two or more independent variables, assuming a linear relationship unlike Simple regression model that models relationship between a dependent and an indendent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23461339-34a5-4a59-84f0-62d7d95fa5d6",
   "metadata": {},
   "source": [
    "#### Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and address this issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831beebc-f6c6-49a4-8c43-43b61e8b5f7f",
   "metadata": {},
   "source": [
    " **Multicollinearity:**\n",
    "   Multicollinearity occurs when two or more independent variables in a multiple linear regression model are highly correlated. This can make it difficult to isolate the individual effects of each variable. It doesn't affect the overall predictive power of the model, but it makes interpreting individual coefficients less reliable. \n",
    "   \n",
    "\n",
    "Multicollinearity can be detected using correlation matrices, variance inflation factor (VIF), and condition indices. To address it, you might consider removing one of the correlated variables or using dimensionality reduction techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adbd24e-b7ba-4e28-b51e-7d942eacea5f",
   "metadata": {},
   "source": [
    "#### Q7. Describe the polynomial regression model. How is it different from linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782d441c-d331-4c7d-a676-23bca7fed5f6",
   "metadata": {},
   "source": [
    "**Polynomial Regression:**\n",
    "   Polynomial regression is an extension of linear regression where the relationship between the dependent and independent variables is modeled as an nth-degree polynomial. The model equation becomes y = b0 + b1x + b2x^2 + ... + bnx^n.\n",
    "   \n",
    "It is different from linear regression as it allows more flexible curve fitting compared to simple or multiple linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c28680-576b-42dd-a54b-833567577078",
   "metadata": {},
   "source": [
    "#### Q8. What are the advantages and disadvantages of polynomial regression compared to linear regression? In what situations would you prefer to use polynomial regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab734113-81a5-460b-8aa7-d808a9dd12cf",
   "metadata": {},
   "source": [
    "   **Advantages:**\n",
    "   - Can capture complex, nonlinear relationships.\n",
    "   - Provides a better fit for data with curvatures.\n",
    "   \n",
    "   **Disadvantages:**\n",
    "   - Prone to overfitting, especially with higher-degree polynomials.\n",
    "   - Can lead to instability in predictions outside the range of observed data.\n",
    "   \n",
    "   \n",
    "Polynomial regression is useful when there's evidence that the relationship between variables is nonlinear and simple linear regression doesn't fit well. However, caution is needed to prevent overfitting, and other techniques like regularization can be employed to manage this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9c3eae-1222-4ac0-93db-1f01a78958ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
